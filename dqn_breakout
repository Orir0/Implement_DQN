from torch import nn
import torch
import gymnasium as gym
from collections import deque
import itertools
import random
import numpy as np
import ale_py 
from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation
import os
import env_preprocessing
from convolutionnet import ConvolutionalNetwork 


# Create saved_models directory if it doesn't exist
os.makedirs('saved_models', exist_ok=True)

GAMMA = 0.99 # dicount factor
BATCH_SIZE = 32 
BUFFER_SIZE = 1000000 # this is the replay buffer - SGD updates are sampled from this number of most recent frames
MIN_REPLAY_SIZE = 50000 # Uniform random policy will run for this number of frames before learning starts and the
#resultin experience is used to populate the replay memory
AGNET_HISTROY_LEN = 4 # the number of most recent frames experienced by the agent that are given as input to the Q network
EPSILON_START = 1.0 
EPSILON_END = 0.1
EPSILON_DECAY = 1000000
TARGET_UPDATE_FREQ = 10000  #every 10000 steps the target neworks update
ACTION_REPEAT = 4 # the agent will repeat the same action for this number of frames
UPDATE_FREQUENCY = 4 # the number of steps between SGD update
LEARNING_RATE = 0.00025 # the learning rate for the optimizer
GRADIENT_MOMENTUM = 0.95
MIN_SQUARED_GRADIENT = 0.01 
INITIAL_EXPLORATION = 1 #initial value of eps in eps-greedy exploration
FINAL_EXPLORATION = 0.1 #final value of eps 
NO_OP_MAX = 30 # max number of 'do nothing' actions to be performed by the agent at the start of an episode


env = env_preprocessing.make_atari_breakout_enviroment_for_dqn()


    

#Setting up the replay buffer & reward buffer for tracking
replay_buffer = deque(maxlen = BUFFER_SIZE)
epinfo_buffer = deque([], maxlen =100)
episode_count = 0
episode_reward =0


online_net = ConvolutionalNetwork(env)
target_net = ConvolutionalNetwork(env)

# Move networks to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
online_net = online_net.to(device)
target_net = target_net.to(device)

# Set networks to train mode
online_net.train()
target_net.train()

# Initialize target network with online network weights

target_net.load_state_dict(online_net.state_dict())

#As the paper stated we'll use RMSprop optimizer
optimizer = torch.optim.RMSprop(
    online_net.parameters(),
    lr=LEARNING_RATE,           # 0.00025
    alpha=GRADIENT_MOMENTUM,    # 0.95 (decay factor)
    eps=MIN_SQUARED_GRADIENT,   # 0.01 (epsilon)
    momentum=0,                 # RMSprop doesn't use momentum
    centered=False
)




#MAIN TRAINING LOOP
obs, info = env.reset()
curr_life = info['lives']
for step in itertools.count():
    epsilon = np.interp(step, [0, EPSILON_DECAY], [EPSILON_START, EPSILON_END])

    random_sample = random.random()

    # Choose action (explore or exploit)
    if random_sample <= epsilon:
        action = env.action_space.sample()
    else:
        action = online_net.act(obs, device)
    
    # Always step the environment
    new_obs, rew, done, truncated, info = env.step(action)
    # Clip rewards as per DQN paper: positive rewards clipped to +1, negative to -1, 0 unchanged
    rew = np.clip(rew, -1, 1)
    episode_reward += rew
    
    
    # Save the transition from the agent's action
    transition = (obs, action, rew, done, new_obs)
    replay_buffer.append(transition)
    obs = new_obs
  
    # If All lives lost beging new game
    if done or truncated:
        epinfo_buffer.append(episode_reward)
        episode_reward = 0
        episode_count += 1
        obs, info = env.reset()
        for _ in range(random.randint(1,30)):
                obs, _, _, _, _ = env.step(1)
    
    # If we lost one live and still have some remaining we need to fire the ball into play
    if info['lives'] < curr_life:
        curr_life = info['lives']
        for _ in range(random.randint(1,30)):
                obs, _, _, _, _ = env.step(1)
    
    # Only train if we have enough samples and it's time to update
    if len(replay_buffer) < MIN_REPLAY_SIZE:
        continue
    
    # number of steps between SGD step
    if step % UPDATE_FREQUENCY != 0:
        continue
    
    transitions = random.sample(replay_buffer, BATCH_SIZE)

    obses = np.asarray([t[0] for t in transitions])
    actions = np.asarray([t[1] for t in transitions])
    rews = np.asarray([t[2] for t in transitions])
    dones = np.asarray([t[3] for t in transitions])
    new_obses = np.asarray([t[4] for t in transitions])

    obses_t = torch.as_tensor(obses,dtype=torch.float32).to(device)
    actions_t = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1).to(device)
    rews_t = torch.as_tensor(rews, dtype=torch.float32).unsqueeze(-1).to(device)
    dones_t = torch.as_tensor(dones, dtype= torch.float32).unsqueeze(-1).to(device)
    new_obses_t = torch.as_tensor(new_obses, dtype=torch.float32).to(device)


    target_q_values = target_net(new_obses_t)
    max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]
    
    targets = rews_t + GAMMA*(1-dones_t)*max_target_q_values

    #comptue loss
    with torch.no_grad():
        q_values = online_net(obses_t)

    action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)

    loss = nn.functional.smooth_l1_loss(action_q_values, targets)

    #gradient descent step
    optimizer.zero_grad()
    loss.backward()
    # Gradient clipping for stability (as per DQN paper)
    torch.nn.utils.clip_grad_norm_(online_net.parameters(), max_norm=10.0)
    optimizer.step()
    
    # Store loss and Q-value stats for logging
    last_loss = loss.item()
    with torch.no_grad():
        last_q_mean = q_values.mean().item()
        last_q_max = q_values.max().item()
        last_target_mean = targets.mean().item()

    #update target network (but not at step 0, since we already initialized it)
    if step > 0 and step % TARGET_UPDATE_FREQ == 0:
        target_net.load_state_dict(online_net.state_dict())
        print(f"Target network updated at step {step}")
        
    if step % 100000 == 0 and step != 0:
        # Save model checkpoint
        checkpoint_path = f'saved_models/checkpoint_step_{step}.pth'
        
        torch.save({
            'step': step,
            'online_net_state_dict': online_net.state_dict(),
            'target_net_state_dict': target_net.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
        }, checkpoint_path)
        print(f"Model saved to: {checkpoint_path}")

    #logging
    if step % 1000 == 0 and step !=0:
        rew_mean = np.mean(epinfo_buffer) 
        print()
        print(f"step: {step}")
        print(f'average reward: {rew_mean}')
        print(f'episode count: {episode_count}')

